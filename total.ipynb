{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2e74d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Researcher\\Desktop\\sesac_hackathon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a7e17a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, Gemma3ForCausalLM\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mweights/gemma3\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Researcher\\Desktop\\sesac_hackathon\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Researcher\\Desktop\\sesac_hackathon\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroi_align\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Researcher\\Desktop\\sesac_hackathon\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[39m, in \u001b[36mregister_meta.<locals>.wrapper\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(fn):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension\u001b[49m._has_ops():\n\u001b[32m     19\u001b[39m         get_meta_lib().impl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch.ops.torchvision, op_name), overload_name), fn)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"weights/gemma3\")\n",
    "model = Gemma3ForCausalLM.from_pretrained('weights/gemma3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd4a7639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "누구야?\n",
      "\n",
      "이 질문에 대한 답은 \"그만 말해\"입니다.\n",
      "\n",
      "**설명:**\n",
      "\n",
      "이 질문은 꽤 유명한 \"그만 말해\" 퀴즈입니다. 이 질문은 1980년대 초에 춤을 추는 동안 댄서들이 서로에게 말했습니다. 그들은 말하는 것을 멈추고, 그들의 춤에 집중했습니다. 질문은 그들이 멈추지 않도록 유도\n"
     ]
    }
   ],
   "source": [
    "prompt = \"누구야?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=100)\n",
    "result = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e5a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['말 빠르기와 피치를 조절해 보겠습니다.']\n",
      " > Processing time: 1.501847505569458\n",
      " > Real-time factor: 0.283641714898302\n"
     ]
    }
   ],
   "source": [
    "from torch.serialization import add_safe_globals\n",
    "from TTS.tts.configs.xtts_config import XttsConfig, XttsAudioConfig  # <- 허용할 클래스 임포트\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.xtts_config import XttsArgs\n",
    "add_safe_globals([BaseDatasetConfig, XttsAudioConfig, XttsArgs])  # <- allowlist 등록\n",
    "\n",
    "from TTS.api import TTS\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "wav = tts.tts(\n",
    "    text=\"말 빠르기와 피치를 조절해 보겠습니다.\",\n",
    "    speaker_wav=\"test.wav\",\n",
    "    language=\"ko\",\n",
    ")\n",
    "\n",
    "wav_tensor = torch.tensor(wav).unsqueeze(0)  # [1, T]\n",
    "sr = 22050\n",
    "\n",
    "from torchaudio.functional import pitch_shift\n",
    "\n",
    "shifted = pitch_shift(wav_tensor, sample_rate=sr, n_steps=-1)  # +2 semitones\n",
    "torchaudio.save(\"out_pitch_minus1_speed_3.wav\", shifted, sr)\n",
    "# sf.write(\"out_speed_10.wav\", wav, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d3ba6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['아, 가시라고요.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Researcher\\Desktop\\sesac_hackathon\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 1.132260799407959\n",
      " > Real-time factor: 0.29106452419028045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Researcher\\Desktop\\sesac_hackathon\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "wav = tts.tts(\n",
    "    text=\"아, 가시라고요.\",\n",
    "    speaker_wav=\"test2.wav\",\n",
    "    language=\"ko\",\n",
    ")\n",
    "\n",
    "sr = 22050\n",
    "wav_tensor = torch.tensor(wav).unsqueeze(0)\n",
    "torchaudio.save(\"output.wav\", wav_tensor, sr)\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# speed = 1.3  # 1.3배 빠르게\n",
    "\n",
    "# subprocess.run([\n",
    "#     \"ffmpeg\",\n",
    "#     \"-y\",                 # 덮어쓰기\n",
    "#     \"-i\", \"temp_tts.wav\",\n",
    "#     \"-filter:a\", f\"atempo={speed}\",\n",
    "#     \"-vn\",\n",
    "#     \"out_speed_1.3_ffmpeg.wav\"\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e8bc2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Init TTS with the target model name\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tts \u001b[38;5;241m=\u001b[39m \u001b[43mTTS\u001b[49m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts_models/de/thorsten/tacotron2-DDC\u001b[39m\u001b[38;5;124m\"\u001b[39m, progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Run TTS\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tts\u001b[38;5;241m.\u001b[39mtts_to_file(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIch bin eine Testnachricht.\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_path\u001b[38;5;241m=\u001b[39mOUTPUT_PATH)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TTS' is not defined"
     ]
    }
   ],
   "source": [
    "# Init TTS with the target model name\n",
    "tts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\n",
    "\n",
    "# Run TTS\n",
    "tts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)\n",
    "\n",
    "# Example voice cloning with YourTTS in English, French and Portuguese\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False).to(device)\n",
    "tts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n",
    "tts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\n",
    "tts.tts_to_file(\"Isso é clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803775d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m model, diffusion = build_model(\u001b[33m'\u001b[39m\u001b[33mconfigs/finetune.yaml\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models import build_model\n",
    "import torch\n",
    "\n",
    "model, diffusion = build_model('configs/finetune.yaml')\n",
    "model.load_state_dict(torch.load('pretrained/styletts2.pt'))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
